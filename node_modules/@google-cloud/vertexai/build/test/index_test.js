"use strict";
/**
 * @license
 * Copyright 2023 Google LLC
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     https://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
Object.defineProperty(exports, "__esModule", { value: true });
exports.testGeneratorWithEmptyResponse = exports.testGenerator = void 0;
/* tslint:disable */
const index_1 = require("../src/index");
const StreamFunctions = require("../src/process_stream");
const content_1 = require("../src/types/content");
const errors_1 = require("../src/types/errors");
const util_1 = require("../src/util");
const PROJECT = 'test_project';
const LOCATION = 'test_location';
const TEST_CHAT_MESSSAGE_TEXT = 'How are you doing today?';
const TEST_USER_CHAT_MESSAGE = [
    { role: util_1.constants.USER_ROLE, parts: [{ text: TEST_CHAT_MESSSAGE_TEXT }] },
];
const TEST_TOKEN = 'testtoken';
const TEST_USER_CHAT_MESSAGE_WITH_GCS_FILE = [
    {
        role: util_1.constants.USER_ROLE,
        parts: [
            { text: TEST_CHAT_MESSSAGE_TEXT },
            {
                file_data: {
                    file_uri: 'gs://test_bucket/test_image.jpeg',
                    mime_type: 'image/jpeg',
                },
            },
        ],
    },
];
const TEST_USER_CHAT_MESSAGE_WITH_INVALID_GCS_FILE = [
    {
        role: util_1.constants.USER_ROLE,
        parts: [
            { text: TEST_CHAT_MESSSAGE_TEXT },
            { file_data: { file_uri: 'test_image.jpeg', mime_type: 'image/jpeg' } },
        ],
    },
];
const TEST_SAFETY_SETTINGS = [
    {
        category: content_1.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
        threshold: content_1.HarmBlockThreshold.BLOCK_ONLY_HIGH,
    },
];
const TEST_SAFETY_RATINGS = [
    {
        category: content_1.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
        probability: content_1.HarmProbability.NEGLIGIBLE,
    },
];
const TEST_GENERATION_CONFIG = {
    candidate_count: 1,
    stop_sequences: ['hello'],
};
const TEST_CANDIDATES = [
    {
        index: 1,
        content: {
            role: util_1.constants.MODEL_ROLE,
            parts: [{ text: 'Im doing great! How are you?' }],
        },
        finishReason: content_1.FinishReason.STOP,
        finishMessage: '',
        safetyRatings: TEST_SAFETY_RATINGS,
        citationMetadata: {
            citationSources: [
                {
                    startIndex: 367,
                    endIndex: 491,
                    uri: 'https://www.numerade.com/ask/question/why-does-the-uncertainty-principle-make-it-impossible-to-predict-a-trajectory-for-the-clectron-95172/',
                },
            ],
        },
    },
];
const TEST_MODEL_RESPONSE = {
    candidates: TEST_CANDIDATES,
    usage_metadata: { prompt_token_count: 0, candidates_token_count: 0 },
};
const TEST_FUNCTION_CALL_RESPONSE = {
    functionCall: {
        name: 'get_current_weather',
        args: {
            location: 'LA',
            unit: 'fahrenheit',
        },
    },
};
const TEST_CANDIDATES_WITH_FUNCTION_CALL = [
    {
        index: 1,
        content: {
            role: util_1.constants.MODEL_ROLE,
            parts: [TEST_FUNCTION_CALL_RESPONSE],
        },
        finishReason: content_1.FinishReason.STOP,
        finishMessage: '',
        safetyRatings: TEST_SAFETY_RATINGS,
    },
];
const TEST_MODEL_RESPONSE_WITH_FUNCTION_CALL = {
    candidates: TEST_CANDIDATES_WITH_FUNCTION_CALL,
};
const TEST_FUNCTION_RESPONSE_PART = [
    {
        functionResponse: {
            name: 'get_current_weather',
            response: { name: 'get_current_weather', content: { weather: 'super nice' } },
        },
    },
];
const TEST_CANDIDATES_MISSING_ROLE = [
    {
        index: 1,
        content: { parts: [{ text: 'Im doing great! How are you?' }] },
        finish_reason: 0,
        finish_message: '',
        safety_ratings: TEST_SAFETY_RATINGS,
    },
];
const TEST_MODEL_RESPONSE_MISSING_ROLE = {
    candidates: TEST_CANDIDATES_MISSING_ROLE,
    usage_metadata: { prompt_token_count: 0, candidates_token_count: 0 },
};
const TEST_EMPTY_MODEL_RESPONSE = {
    candidates: [],
};
const TEST_ENDPOINT_BASE_PATH = 'test.googleapis.com';
const TEST_GCS_FILENAME = 'gs://test_bucket/test_image.jpeg';
const TEST_MULTIPART_MESSAGE = [
    {
        role: util_1.constants.USER_ROLE,
        parts: [
            { text: 'What is in this picture?' },
            { file_data: { file_uri: TEST_GCS_FILENAME, mime_type: 'image/jpeg' } },
        ],
    },
];
const BASE_64_IMAGE = 'iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M9QDwADhgGAWjR9awAAAABJRU5ErkJggg==';
const INLINE_DATA_FILE_PART = {
    inline_data: {
        data: BASE_64_IMAGE,
        mime_type: 'image/jpeg',
    },
};
const TEST_MULTIPART_MESSAGE_BASE64 = [
    {
        role: util_1.constants.USER_ROLE,
        parts: [{ text: 'What is in this picture?' }, INLINE_DATA_FILE_PART],
    },
];
const TEST_TOOLS_WITH_FUNCTION_DECLARATION = [
    {
        function_declarations: [
            {
                name: 'get_current_weather',
                description: 'get weather in a given location',
                parameters: {
                    type: content_1.FunctionDeclarationSchemaType.OBJECT,
                    properties: {
                        location: { type: content_1.FunctionDeclarationSchemaType.STRING },
                        unit: {
                            type: content_1.FunctionDeclarationSchemaType.STRING,
                            enum: ['celsius', 'fahrenheit'],
                        },
                    },
                    required: ['location'],
                },
            },
        ],
    },
];
const fetchResponseObj = {
    status: 200,
    statusText: 'OK',
    ok: true,
    headers: { 'Content-Type': 'application/json' },
    url: 'url',
};
/**
 * Returns a generator, used to mock the generateContentStream response
 * @ignore
 */
async function* testGenerator() {
    yield {
        candidates: TEST_CANDIDATES,
    };
}
exports.testGenerator = testGenerator;
async function* testGeneratorWithEmptyResponse() {
    yield {
        candidates: [],
    };
}
exports.testGeneratorWithEmptyResponse = testGeneratorWithEmptyResponse;
describe('VertexAI', () => {
    let vertexai;
    let model;
    let expectedStreamResult;
    let fetchSpy;
    beforeEach(() => {
        vertexai = new index_1.VertexAI({
            project: PROJECT,
            location: LOCATION,
        });
        spyOnProperty(vertexai.preview, 'token', 'get').and.resolveTo(TEST_TOKEN);
        model = vertexai.preview.getGenerativeModel({ model: 'gemini-pro' });
        expectedStreamResult = {
            response: Promise.resolve(TEST_MODEL_RESPONSE),
            stream: testGenerator(),
        };
        const fetchResult = new Response(JSON.stringify(expectedStreamResult), fetchResponseObj);
        fetchSpy = spyOn(global, 'fetch').and.resolveTo(fetchResult);
    });
    it('given undefined google auth options, should be instantiated', () => {
        expect(vertexai).toBeInstanceOf(index_1.VertexAI);
    });
    it('given specified google auth options, should be instantiated', () => {
        const googleAuthOptions = {
            scopes: 'https://www.googleapis.com/auth/cloud-platform',
        };
        const vetexai1 = new index_1.VertexAI({
            project: PROJECT,
            location: LOCATION,
            googleAuthOptions: googleAuthOptions,
        });
        expect(vetexai1).toBeInstanceOf(index_1.VertexAI);
    });
    it('given inconsistent project ID, should throw error', () => {
        const googleAuthOptions = {
            projectId: 'another_project',
        };
        expect(() => {
            new index_1.VertexAI({
                project: PROJECT,
                location: LOCATION,
                googleAuthOptions: googleAuthOptions,
            });
        }).toThrow(new Error('inconsistent project ID values. argument project got value test_project but googleAuthOptions.projectId got value another_project'));
    });
    it('given scopes missing required scope, should throw GoogleAuthError', () => {
        const invalidGoogleAuthOptionsStringScopes = { scopes: 'test.scopes' };
        expect(() => {
            new index_1.VertexAI({
                project: PROJECT,
                location: LOCATION,
                googleAuthOptions: invalidGoogleAuthOptionsStringScopes,
            });
        }).toThrow(new errors_1.GoogleAuthError("input GoogleAuthOptions.scopes test.scopes doesn't contain required scope " +
            'https://www.googleapis.com/auth/cloud-platform, ' +
            'please include https://www.googleapis.com/auth/cloud-platform into GoogleAuthOptions.scopes ' +
            'or leave GoogleAuthOptions.scopes undefined'));
        const invalidGoogleAuthOptionsArrayScopes = {
            scopes: ['test1.scopes', 'test2.scopes'],
        };
        expect(() => {
            new index_1.VertexAI({
                project: PROJECT,
                location: LOCATION,
                googleAuthOptions: invalidGoogleAuthOptionsArrayScopes,
            });
        }).toThrow(new errors_1.GoogleAuthError("input GoogleAuthOptions.scopes test1.scopes,test2.scopes doesn't contain required scope " +
            'https://www.googleapis.com/auth/cloud-platform, ' +
            'please include https://www.googleapis.com/auth/cloud-platform into GoogleAuthOptions.scopes ' +
            'or leave GoogleAuthOptions.scopes undefined'));
    });
    describe('generateContent', () => {
        it('returns a GenerateContentResponse', async () => {
            const req = {
                contents: TEST_USER_CHAT_MESSAGE,
            };
            const expectedResult = {
                response: TEST_MODEL_RESPONSE,
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedStreamResult);
            const resp = await model.generateContent(req);
            expect(resp).toEqual(expectedResult);
        });
        it('returns a GenerateContentResponse when passed a string', async () => {
            const expectedResult = {
                response: TEST_MODEL_RESPONSE,
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedStreamResult);
            const resp = await model.generateContent(TEST_CHAT_MESSSAGE_TEXT);
            expect(resp).toEqual(expectedResult);
        });
        it('returns a GenerateContentResponse when passed a GCS URI', async () => {
            const req = {
                contents: TEST_USER_CHAT_MESSAGE_WITH_GCS_FILE,
            };
            const expectedResult = {
                response: TEST_MODEL_RESPONSE,
            };
            const expectedStreamResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE),
                stream: testGenerator(),
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedStreamResult);
            const resp = await model.generateContent(req);
            expect(resp).toEqual(expectedResult);
        });
        it('raises an error when passed an invalid GCS URI', async () => {
            const req = {
                contents: TEST_USER_CHAT_MESSAGE_WITH_INVALID_GCS_FILE,
            };
            await expectAsync(model.generateContent(req)).toBeRejectedWithError(URIError);
        });
        it('returns a GenerateContentResponse when passed safety_settings and generation_config', async () => {
            const req = {
                contents: TEST_USER_CHAT_MESSAGE,
                safety_settings: TEST_SAFETY_SETTINGS,
                generation_config: TEST_GENERATION_CONFIG,
            };
            const expectedResult = {
                response: TEST_MODEL_RESPONSE,
            };
            const expectedStreamResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE),
                stream: testGenerator(),
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedStreamResult);
            const resp = await model.generateContent(req);
            expect(resp).toEqual(expectedResult);
        });
        it('updates the base API endpoint when provided', async () => {
            const vertexaiWithBasePath = new index_1.VertexAI({
                project: PROJECT,
                location: LOCATION,
                apiEndpoint: TEST_ENDPOINT_BASE_PATH,
            });
            spyOnProperty(vertexaiWithBasePath.preview, 'token', 'get').and.resolveTo(TEST_TOKEN);
            model = vertexaiWithBasePath.preview.getGenerativeModel({
                model: 'gemini-pro',
            });
            const req = {
                contents: TEST_USER_CHAT_MESSAGE,
            };
            const expectedResult = {
                response: TEST_MODEL_RESPONSE,
            };
            const expectedStreamResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE),
                stream: testGenerator(),
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedStreamResult);
            await model.generateContent(req);
            expect(fetchSpy.calls.allArgs()[0][0].toString()).toContain(TEST_ENDPOINT_BASE_PATH);
        });
        it('default the base API endpoint when base API not provided', async () => {
            const vertexaiWithoutBasePath = new index_1.VertexAI({
                project: PROJECT,
                location: LOCATION,
            });
            spyOnProperty(vertexaiWithoutBasePath.preview, 'token', 'get').and.resolveTo(TEST_TOKEN);
            model = vertexaiWithoutBasePath.preview.getGenerativeModel({
                model: 'gemini-pro',
            });
            const req = {
                contents: TEST_USER_CHAT_MESSAGE,
            };
            const expectedResult = {
                response: TEST_MODEL_RESPONSE,
            };
            const expectedStreamResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE),
                stream: testGenerator(),
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedStreamResult);
            await model.generateContent(req);
            expect(fetchSpy.calls.allArgs()[0][0].toString()).toContain(`${LOCATION}-aiplatform.googleapis.com`);
        });
        it('removes top_k when it is set to 0', async () => {
            const reqWithEmptyConfigs = {
                contents: TEST_USER_CHAT_MESSAGE_WITH_GCS_FILE,
                generation_config: { top_k: 0 },
                safety_settings: [],
            };
            const expectedResult = {
                response: TEST_MODEL_RESPONSE,
            };
            const expectedStreamResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE),
                stream: testGenerator(),
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedStreamResult);
            await model.generateContent(reqWithEmptyConfigs);
            const requestArgs = fetchSpy.calls.allArgs()[0][1];
            if (typeof requestArgs === 'object' && requestArgs) {
                expect(JSON.stringify(requestArgs['body'])).not.toContain('top_k');
            }
        });
        it('includes top_k when it is within 1 - 40', async () => {
            const reqWithEmptyConfigs = {
                contents: TEST_USER_CHAT_MESSAGE_WITH_GCS_FILE,
                generation_config: { top_k: 1 },
                safety_settings: [],
            };
            const expectedResult = {
                response: TEST_MODEL_RESPONSE,
            };
            const expectedStreamResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE),
                stream: testGenerator(),
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedStreamResult);
            await model.generateContent(reqWithEmptyConfigs);
            const requestArgs = fetchSpy.calls.allArgs()[0][1];
            if (typeof requestArgs === 'object' && requestArgs) {
                expect(JSON.stringify(requestArgs['body'])).toContain('top_k');
            }
        });
        it('aggregates citation metadata', async () => {
            var _a;
            const req = {
                contents: TEST_USER_CHAT_MESSAGE,
            };
            const expectedResult = {
                response: TEST_MODEL_RESPONSE,
            };
            const expectedStreamResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE),
                stream: testGenerator(),
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedStreamResult);
            const resp = await model.generateContent(req);
            expect((_a = resp.response.candidates[0].citationMetadata) === null || _a === void 0 ? void 0 : _a.citationSources.length).toEqual(TEST_MODEL_RESPONSE.candidates[0].citationMetadata.citationSources
                .length);
        });
        it('returns a FunctionCall when passed a FunctionDeclaration', async () => {
            const req = {
                contents: [
                    { role: 'user', parts: [{ text: 'What is the weater like in Boston?' }] },
                ],
                tools: TEST_TOOLS_WITH_FUNCTION_DECLARATION,
            };
            const expectedResult = {
                response: TEST_MODEL_RESPONSE_WITH_FUNCTION_CALL,
            };
            const expectedStreamResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE_WITH_FUNCTION_CALL),
                stream: testGenerator(),
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedStreamResult);
            const resp = await model.generateContent(req);
            expect(resp).toEqual(expectedResult);
        });
        it('throws ClientError when functionResponse is not immedidately following functionCall case1', async () => {
            const req = {
                contents: [
                    { role: 'user', parts: [{ text: 'What is the weater like in Boston?' }] },
                    {
                        role: 'function',
                        parts: TEST_FUNCTION_RESPONSE_PART,
                    },
                ],
                tools: TEST_TOOLS_WITH_FUNCTION_DECLARATION,
            };
            const expectedErrorMessage = '[VertexAI.ClientError]: Please ensure that function response turn comes immediately after a function call turn.';
            await model.generateContent(req).catch(e => {
                expect(e.message).toEqual(expectedErrorMessage);
            });
        });
        it('throws ClientError when functionResponse is not immedidately following functionCall case2', async () => {
            const req = {
                contents: [
                    { role: 'user', parts: [{ text: 'What is the weater like in Boston?' }] },
                    {
                        role: 'function',
                        parts: TEST_FUNCTION_RESPONSE_PART,
                    },
                ],
                tools: TEST_TOOLS_WITH_FUNCTION_DECLARATION,
            };
            const expectedErrorMessage = '[VertexAI.ClientError]: Please ensure that function response turn comes immediately after a function call turn.';
            await model.generateContent(req).catch(e => {
                expect(e.message).toEqual(expectedErrorMessage);
            });
        });
    });
    describe('generateContentStream', () => {
        it('returns a GenerateContentResponse when passed text content', async () => {
            const req = {
                contents: TEST_USER_CHAT_MESSAGE,
            };
            const expectedResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE),
                stream: testGenerator(),
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedResult);
            const resp = await model.generateContentStream(req);
            expect(resp).toEqual(expectedResult);
        });
        it('returns a GenerateContentResponse when passed a string', async () => {
            const expectedResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE),
                stream: testGenerator(),
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedResult);
            const resp = await model.generateContentStream(TEST_CHAT_MESSSAGE_TEXT);
            expect(resp).toEqual(expectedResult);
        });
        it('returns a GenerateContentResponse when passed multi-part content with a GCS URI', async () => {
            const req = {
                contents: TEST_MULTIPART_MESSAGE,
            };
            const expectedResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE),
                stream: testGenerator(),
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedResult);
            const resp = await model.generateContentStream(req);
            expect(resp).toEqual(expectedResult);
        });
        it('returns a GenerateContentResponse when passed multi-part content with base64 data', async () => {
            const req = {
                contents: TEST_MULTIPART_MESSAGE_BASE64,
            };
            const expectedResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE),
                stream: testGenerator(),
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedResult);
            const resp = await model.generateContentStream(req);
            expect(resp).toEqual(expectedResult);
        });
        it('returns a FunctionCall when passed a FunctionDeclaration', async () => {
            const req = {
                contents: [
                    { role: 'user', parts: [{ text: 'What is the weater like in Boston?' }] },
                ],
                tools: TEST_TOOLS_WITH_FUNCTION_DECLARATION,
            };
            const expectedStreamResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE_WITH_FUNCTION_CALL),
                stream: testGenerator(),
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedStreamResult);
            const resp = await model.generateContentStream(req);
            expect(resp).toEqual(expectedStreamResult);
        });
        it('throws ClientError when functionResponse is not immedidately following functionCall case1', async () => {
            const req = {
                contents: [
                    { role: 'user', parts: [{ text: 'What is the weater like in Boston?' }] },
                    {
                        role: 'function',
                        parts: TEST_FUNCTION_RESPONSE_PART,
                    },
                ],
                tools: TEST_TOOLS_WITH_FUNCTION_DECLARATION,
            };
            const expectedErrorMessage = '[VertexAI.ClientError]: Please ensure that function response turn comes immediately after a function call turn.';
            await model.generateContentStream(req).catch(e => {
                expect(e.message).toEqual(expectedErrorMessage);
            });
        });
        it('throws ClientError when functionResponse is not immedidately following functionCall case2', async () => {
            const req = {
                contents: [
                    { role: 'user', parts: [{ text: 'What is the weater like in Boston?' }] },
                    {
                        role: 'function',
                        parts: TEST_FUNCTION_RESPONSE_PART,
                    },
                ],
                tools: TEST_TOOLS_WITH_FUNCTION_DECLARATION,
            };
            const expectedErrorMessage = '[VertexAI.ClientError]: Please ensure that function response turn comes immediately after a function call turn.';
            await model.generateContentStream(req).catch(e => {
                expect(e.message).toEqual(expectedErrorMessage);
            });
        });
    });
    describe('startChat', () => {
        it('returns a ChatSession when passed a request arg', () => {
            const req = {
                history: TEST_USER_CHAT_MESSAGE,
            };
            const resp = model.startChat(req);
            expect(resp).toBeInstanceOf(index_1.ChatSession);
        });
        it('returns a ChatSession when passed no request arg', () => {
            const resp = model.startChat();
            expect(resp).toBeInstanceOf(index_1.ChatSession);
        });
    });
});
describe('countTokens', () => {
    it('returns the token count', async () => {
        const vertexai = new index_1.VertexAI({
            project: PROJECT,
            location: LOCATION,
        });
        spyOnProperty(vertexai.preview, 'token', 'get').and.resolveTo(TEST_TOKEN);
        const model = vertexai.preview.getGenerativeModel({ model: 'gemini-pro' });
        const req = {
            contents: TEST_USER_CHAT_MESSAGE,
        };
        const responseBody = {
            totalTokens: 1,
        };
        const response = new Response(JSON.stringify(responseBody), fetchResponseObj);
        spyOn(global, 'fetch').and.resolveTo(response);
        const resp = await model.countTokens(req);
        expect(resp).toEqual(responseBody);
    });
});
describe('ChatSession', () => {
    let chatSession;
    let chatSessionWithNoArgs;
    let chatSessionWithEmptyResponse;
    let chatSessionWithFunctionCall;
    let vertexai;
    let model;
    let expectedStreamResult;
    beforeEach(() => {
        vertexai = new index_1.VertexAI({ project: PROJECT, location: LOCATION });
        spyOnProperty(vertexai.preview, 'token', 'get').and.resolveTo(TEST_TOKEN);
        model = vertexai.preview.getGenerativeModel({ model: 'gemini-pro' });
        chatSession = model.startChat({
            history: TEST_USER_CHAT_MESSAGE,
        });
        expect(chatSession.history).toEqual(TEST_USER_CHAT_MESSAGE);
        chatSessionWithNoArgs = model.startChat();
        chatSessionWithEmptyResponse = model.startChat();
        chatSessionWithFunctionCall = model.startChat({
            tools: TEST_TOOLS_WITH_FUNCTION_DECLARATION,
        });
        expectedStreamResult = {
            response: Promise.resolve(TEST_MODEL_RESPONSE),
            stream: testGenerator(),
        };
        const fetchResult = Promise.resolve(new Response(JSON.stringify(expectedStreamResult), fetchResponseObj));
        spyOn(global, 'fetch').and.returnValue(fetchResult);
    });
    describe('sendMessage', () => {
        it('returns a GenerateContentResponse and appends to history', async () => {
            const req = 'How are you doing today?';
            const expectedResult = {
                response: TEST_MODEL_RESPONSE,
            };
            const expectedStreamResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE),
                stream: testGenerator(),
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedStreamResult);
            const resp = await chatSession.sendMessage(req);
            expect(resp).toEqual(expectedResult);
            expect(chatSession.history.length).toEqual(3);
        });
        it('returns a GenerateContentResponse and appends to history when startChat is passed with no args', async () => {
            const req = 'How are you doing today?';
            const expectedResult = {
                response: TEST_MODEL_RESPONSE,
            };
            const expectedStreamResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE),
                stream: testGenerator(),
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedStreamResult);
            const resp = await chatSessionWithNoArgs.sendMessage(req);
            expect(resp).toEqual(expectedResult);
            expect(chatSessionWithNoArgs.history.length).toEqual(2);
        });
        it('throws an error when the model returns an empty response', async () => {
            const req = 'How are you doing today?';
            const expectedResult = {
                response: TEST_EMPTY_MODEL_RESPONSE,
            };
            const expectedStreamResult = {
                response: Promise.resolve(TEST_EMPTY_MODEL_RESPONSE),
                stream: testGeneratorWithEmptyResponse(),
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedStreamResult);
            await expectAsync(chatSessionWithEmptyResponse.sendMessage(req)).toBeRejected();
            expect(chatSessionWithEmptyResponse.history.length).toEqual(0);
        });
        it('returns a GenerateContentResponse when passed multi-part content', async () => {
            const req = TEST_MULTIPART_MESSAGE[0]['parts'];
            const expectedResult = {
                response: TEST_MODEL_RESPONSE,
            };
            const expectedStreamResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE),
                stream: testGenerator(),
            };
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedStreamResult);
            const resp = await chatSessionWithNoArgs.sendMessage(req);
            expect(resp).toEqual(expectedResult);
            console.log(chatSessionWithNoArgs.history, 'hihii');
            expect(chatSessionWithNoArgs.history.length).toEqual(2);
        });
        it('returns a FunctionCall and appends to history when passed a FunctionDeclaration', async () => {
            const functionCallChatMessage = 'What is the weather in LA?';
            const expectedFunctionCallResponse = {
                response: TEST_MODEL_RESPONSE_WITH_FUNCTION_CALL,
            };
            const expectedStreamResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE_WITH_FUNCTION_CALL),
                stream: testGenerator(),
            };
            const streamSpy = spyOn(StreamFunctions, 'processStream');
            streamSpy.and.returnValue(expectedStreamResult);
            const response1 = await chatSessionWithFunctionCall.sendMessage(functionCallChatMessage);
            expect(response1).toEqual(expectedFunctionCallResponse);
            expect(chatSessionWithFunctionCall.history.length).toEqual(2);
            // Send a follow-up message with a FunctionResponse
            const expectedFollowUpResponse = {
                response: TEST_MODEL_RESPONSE,
            };
            const expectedFollowUpStreamResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE),
                stream: testGenerator(),
            };
            streamSpy.and.returnValue(expectedFollowUpStreamResult);
            const response2 = await chatSessionWithFunctionCall.sendMessage(TEST_FUNCTION_RESPONSE_PART);
            expect(response2).toEqual(expectedFollowUpResponse);
            expect(chatSessionWithFunctionCall.history.length).toEqual(4);
        });
        it('throw ClientError when request has no content', async () => {
            const expectedErrorMessage = '[VertexAI.ClientError]: No content is provided for sending chat message.';
            await chatSessionWithNoArgs.sendMessage([]).catch(e => {
                expect(e.message).toEqual(expectedErrorMessage);
            });
        });
        it('throw ClientError when request mix functionCall part with other types of part', async () => {
            const chatRequest = [
                'what is the weather like in LA',
                TEST_FUNCTION_RESPONSE_PART[0],
            ];
            const expectedErrorMessage = '[VertexAI.ClientError]: Within a single message, FunctionResponse cannot be mixed with other type of part in the request for sending chat message.';
            await chatSessionWithNoArgs.sendMessage(chatRequest).catch(e => {
                expect(e.message).toEqual(expectedErrorMessage);
            });
        });
    });
    describe('sendMessageStream', () => {
        it('returns a StreamGenerateContentResponse and appends to history', async () => {
            const req = 'How are you doing today?';
            const expectedResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE),
                stream: testGenerator(),
            };
            const chatSession = model.startChat({
                history: [
                    {
                        role: util_1.constants.USER_ROLE,
                        parts: [{ text: 'How are you doing today?' }],
                    },
                ],
            });
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedResult);
            expect(chatSession.history.length).toEqual(1);
            expect(chatSession.history[0].role).toEqual(util_1.constants.USER_ROLE);
            const result = await chatSession.sendMessageStream(req);
            const response = await result.response;
            const expectedResponse = await expectedResult.response;
            expect(response).toEqual(expectedResponse);
            expect(chatSession.history.length).toEqual(3);
            expect(chatSession.history[0].role).toEqual(util_1.constants.USER_ROLE);
            expect(chatSession.history[1].role).toEqual(util_1.constants.USER_ROLE);
            expect(chatSession.history[2].role).toEqual(util_1.constants.MODEL_ROLE);
        });
        it('returns a StreamGenerateContentResponse and appends role if missing', async () => {
            const req = 'How are you doing today?';
            const expectedResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE_MISSING_ROLE),
                stream: testGenerator(),
            };
            const chatSession = model.startChat({
                history: [
                    {
                        role: util_1.constants.USER_ROLE,
                        parts: [{ text: 'How are you doing today?' }],
                    },
                ],
            });
            spyOn(StreamFunctions, 'processStream').and.returnValue(expectedResult);
            expect(chatSession.history.length).toEqual(1);
            expect(chatSession.history[0].role).toEqual(util_1.constants.USER_ROLE);
            const result = await chatSession.sendMessageStream(req);
            const response = await result.response;
            const expectedResponse = await expectedResult.response;
            expect(response).toEqual(expectedResponse);
            expect(chatSession.history.length).toEqual(3);
            expect(chatSession.history[0].role).toEqual(util_1.constants.USER_ROLE);
            expect(chatSession.history[1].role).toEqual(util_1.constants.USER_ROLE);
            expect(chatSession.history[2].role).toEqual(util_1.constants.MODEL_ROLE);
        });
        it('returns a FunctionCall and appends to history when passed a FunctionDeclaration', async () => {
            const functionCallChatMessage = 'What is the weather in LA?';
            const expectedStreamResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE_WITH_FUNCTION_CALL),
                stream: testGenerator(),
            };
            const streamSpy = spyOn(StreamFunctions, 'processStream');
            streamSpy.and.returnValue(expectedStreamResult);
            const response1 = await chatSessionWithFunctionCall.sendMessageStream(functionCallChatMessage);
            expect(response1).toEqual(expectedStreamResult);
            expect(chatSessionWithFunctionCall.history.length).toEqual(2);
            // Send a follow-up message with a FunctionResponse
            const expectedFollowUpStreamResult = {
                response: Promise.resolve(TEST_MODEL_RESPONSE),
                stream: testGenerator(),
            };
            streamSpy.and.returnValue(expectedFollowUpStreamResult);
            const response2 = await chatSessionWithFunctionCall.sendMessageStream(TEST_FUNCTION_RESPONSE_PART);
            expect(response2).toEqual(expectedFollowUpStreamResult);
            expect(chatSessionWithFunctionCall.history.length).toEqual(4);
        });
        it('throw ClientError when request has no content', async () => {
            const expectedErrorMessage = '[VertexAI.ClientError]: No content is provided for sending chat message.';
            await chatSessionWithNoArgs.sendMessageStream([]).catch(e => {
                expect(e.message).toEqual(expectedErrorMessage);
            });
        });
        it('throw ClientError when request mix functionCall part with other types of part', async () => {
            const chatRequest = [
                'what is the weather like in LA',
                TEST_FUNCTION_RESPONSE_PART[0],
            ];
            const expectedErrorMessage = '[VertexAI.ClientError]: Within a single message, FunctionResponse cannot be mixed with other type of part in the request for sending chat message.';
            await chatSessionWithNoArgs.sendMessageStream(chatRequest).catch(e => {
                expect(e.message).toEqual(expectedErrorMessage);
            });
        });
    });
});
describe('when exception at fetch', () => {
    const expectedErrorMessage = '[VertexAI.GoogleGenerativeAIError]: exception posting request';
    const vertexai = new index_1.VertexAI({
        project: PROJECT,
        location: LOCATION,
    });
    const model = vertexai.preview.getGenerativeModel({ model: 'gemini-pro' });
    const chatSession = model.startChat();
    const message = 'hi';
    const req = {
        contents: TEST_USER_CHAT_MESSAGE,
    };
    const countTokenReq = {
        contents: TEST_USER_CHAT_MESSAGE,
    };
    beforeEach(() => {
        spyOnProperty(vertexai.preview, 'token', 'get').and.resolveTo(TEST_TOKEN);
        spyOn(global, 'fetch').and.throwError('error');
    });
    it('generateContent should throw GoogleGenerativeAI error', async () => {
        await expectAsync(model.generateContent(req)).toBeRejected();
    });
    it('generateContentStream should throw GoogleGenerativeAI error', async () => {
        await expectAsync(model.generateContentStream(req)).toBeRejected();
    });
    it('sendMessage should throw GoogleGenerativeAI error', async () => {
        await expectAsync(chatSession.sendMessage(message)).toBeRejected();
    });
    it('countTokens should throw GoogleGenerativeAI error', async () => {
        await expectAsync(model.countTokens(countTokenReq)).toBeRejected();
    });
});
describe('when response is undefined', () => {
    const expectedErrorMessage = '[VertexAI.GoogleGenerativeAIError]: response is undefined';
    const vertexai = new index_1.VertexAI({
        project: PROJECT,
        location: LOCATION,
    });
    const model = vertexai.preview.getGenerativeModel({ model: 'gemini-pro' });
    const req = {
        contents: TEST_USER_CHAT_MESSAGE,
    };
    const message = 'hi';
    const chatSession = model.startChat();
    const countTokenReq = {
        contents: TEST_USER_CHAT_MESSAGE,
    };
    beforeEach(() => {
        spyOnProperty(vertexai.preview, 'token', 'get').and.resolveTo(TEST_TOKEN);
        spyOn(global, 'fetch').and.resolveTo();
    });
    it('generateContent should throw GoogleGenerativeAI error', async () => {
        await expectAsync(model.generateContent(req)).toBeRejected();
        await model.generateContent(req).catch(e => {
            expect(e.message).toEqual(expectedErrorMessage);
        });
    });
    it('generateContentStream should throw GoogleGenerativeAI error', async () => {
        await expectAsync(model.generateContentStream(req)).toBeRejected();
        await model.generateContentStream(req).catch(e => {
            expect(e.message).toEqual(expectedErrorMessage);
        });
    });
    it('sendMessage should throw GoogleGenerativeAI error', async () => {
        await expectAsync(chatSession.sendMessage(message)).toBeRejected();
        await chatSession.sendMessage(message).catch(e => {
            expect(e.message).toEqual(expectedErrorMessage);
        });
    });
    it('countTokens should throw GoogleGenerativeAI error', async () => {
        await expectAsync(model.countTokens(countTokenReq)).toBeRejected();
        await model.countTokens(countTokenReq).catch(e => {
            expect(e.message).toEqual(expectedErrorMessage);
        });
    });
});
describe('when response is 4XX', () => {
    const expectedErrorMessage = '[VertexAI.ClientError]: got status: 400 Bad Request';
    const req = {
        contents: TEST_USER_CHAT_MESSAGE,
    };
    const vertexai = new index_1.VertexAI({
        project: PROJECT,
        location: LOCATION,
    });
    const fetch400Obj = {
        status: 400,
        statusText: 'Bad Request',
        ok: false,
    };
    const body = {};
    const response = new Response(JSON.stringify(body), fetch400Obj);
    const model = vertexai.preview.getGenerativeModel({ model: 'gemini-pro' });
    const message = 'hi';
    const chatSession = model.startChat();
    const countTokenReq = {
        contents: TEST_USER_CHAT_MESSAGE,
    };
    beforeEach(() => {
        spyOnProperty(vertexai.preview, 'token', 'get').and.resolveTo(TEST_TOKEN);
        spyOn(global, 'fetch').and.resolveTo(response);
    });
    it('generateContent should throw ClientError error', async () => {
        await expectAsync(model.generateContent(req)).toBeRejected();
        await model.generateContent(req).catch(e => {
            expect(e.message).toEqual(expectedErrorMessage);
        });
    });
    it('generateContentStream should throw ClientError error', async () => {
        await expectAsync(model.generateContentStream(req)).toBeRejected();
        await model.generateContentStream(req).catch(e => {
            expect(e.message).toEqual(expectedErrorMessage);
        });
    });
    it('sendMessage should throw ClientError error', async () => {
        await expectAsync(chatSession.sendMessage(message)).toBeRejected();
        await chatSession.sendMessage(message).catch(e => {
            expect(e.message).toEqual(expectedErrorMessage);
        });
    });
    it('countTokens should throw ClientError error', async () => {
        await expectAsync(model.countTokens(countTokenReq)).toBeRejected();
        await model.countTokens(countTokenReq).catch(e => {
            expect(e.message).toEqual(expectedErrorMessage);
        });
    });
});
describe('when response is not OK and not 4XX', () => {
    const expectedErrorMessage = '[VertexAI.GoogleGenerativeAIError]: got status: 500 Internal Server Error';
    const req = {
        contents: TEST_USER_CHAT_MESSAGE,
    };
    const vertexai = new index_1.VertexAI({
        project: PROJECT,
        location: LOCATION,
    });
    const fetch500Obj = {
        status: 500,
        statusText: 'Internal Server Error',
        ok: false,
    };
    const body = {};
    const response = new Response(JSON.stringify(body), fetch500Obj);
    const model = vertexai.preview.getGenerativeModel({ model: 'gemini-pro' });
    const message = 'hi';
    const chatSession = model.startChat();
    const countTokenReq = {
        contents: TEST_USER_CHAT_MESSAGE,
    };
    beforeEach(() => {
        spyOnProperty(vertexai.preview, 'token', 'get').and.resolveTo(TEST_TOKEN);
        spyOn(global, 'fetch').and.resolveTo(response);
    });
    it('generateContent should throws GoogleGenerativeAIError', async () => {
        await expectAsync(model.generateContent(req)).toBeRejected();
        await model.generateContent(req).catch(e => {
            expect(e.message).toEqual(expectedErrorMessage);
        });
    });
    it('generateContentStream should throws GoogleGenerativeAIError', async () => {
        await expectAsync(model.generateContentStream(req)).toBeRejected();
        await model.generateContentStream(req).catch(e => {
            expect(e.message).toEqual(expectedErrorMessage);
        });
    });
    it('sendMessage should throws GoogleGenerativeAIError', async () => {
        await expectAsync(chatSession.sendMessage(message)).toBeRejected();
        await chatSession.sendMessage(message).catch(e => {
            expect(e.message).toEqual(expectedErrorMessage);
        });
    });
    it('countTokens should throws GoogleGenerativeAIError', async () => {
        await expectAsync(model.countTokens(countTokenReq)).toBeRejected();
        await model.countTokens(countTokenReq).catch(e => {
            expect(e.message).toEqual(expectedErrorMessage);
        });
    });
});
//# sourceMappingURL=index_test.js.map